function hybridv2_lock(the_lock, qnode):
    qnode.locking_id = the_lock.id

    # Try to acquire the lock immediately
    if atomic_test_and_set(the_lock.lock_value, 1) == 0:
        return  # Lock acquired successfully, exit function

    # MCS Lock mechanism
    enqueued = 0
    if not the_lock.preempted_count:
        enqueued = 1

        qnode.next = NULL
        qnode.waiting = 1

        # Exchange qnode and the queue head
        pred = atomic_exchange(the_lock.queue, qnode)
        if pred != NULL:
            pred.next = qnode # Link predecessor to current qnode

            # Spin until lock is acquired or no longer blocking
            while qnode.waiting != 0 and not the_lock.preempted_count:
                PAUSE()

    # Attempt to acquire the lock again with blocking checks
    while atomic_test_and_set(the_lock.lock_value, 1) != 0:
        if the_lock.preempted_count:
            atomic_add(the_lock.waiter_count, 1)
            futex_wait(the_lock.lock_value, 1)
            atomic_sub(the_lock.waiter_count, 1)
        else:
            PAUSE()

    # MCS Unlock mechanism
    if enqueued:
        if qnode.next == NULL:
            # Attempt to reset the queue pointer
            if atomic_compare_and_swap(the_lock.queue, qnode, NULL) == qnode:
                return  # Successfully reset, exit function

            # Spin until successor appears
            while qnode.next == NULL:
                PAUSE()

        # Check if the successor is running
        if not qnode.next.is_running:
            atomic_add(the_lock.preempted_count, 1)

        qnode.next.waiting = 0  # Unlock the successor


function hybridv2_unlock(the_lock):
    if the_lock.preempted_count % 2 != 0: # Fix next holder sleeping detection
        atomic_sub(the_lock.preempted_count, 1)

    # Release the lock
    the_lock.lock_value = 0

    # If there are threads waiting, wake one of them
    if the_lock.waiter_count > 0:
        futex_wake(the_lock.lock_value, 1)

    qnodes[thread_id].locking_id = -1
    
function sched_switch_btf(preempt, prev, next):
    qnode = NULL
    lock_id = 0
    thread_id = NULL

    # Clear preempted status of the next thread if it is not a kernel thread
    if not is_kernel_thread(next):
        thread_id = lookup_thread_id(next.pid)

        if thread_id != -1:
            qnode = qnodes[thread_id]
            qnode.is_running = 1
            lock_id = qnode.locking_id

            # If the thread is holding a lock and was preempted
            if lock_id != -1 and qnode.is_critical_preempted:
                atomic_sub(lock_info[lock_id].preempted_count, 2)
                qnode.is_critical_preempted = 0

    # Optimization: Return early if either prev or next is a kernel thread
    if is_kernel_thread(prev) or is_kernel_thread(next):
        return 0

    # Return early if the previous task is in a non-runnable state
    if not is_runnable_task(prev):
        return 0

    # Retrieve the qnode of the previous task
    thread_id = lookup_thread_id(prev.pid)
    
    if thread_id == -1:
        return 0

    qnode = qnodes[thread_id]
    qnode.is_running = 0
    lock_id = qnode.locking_id

    # Ignore preemption if the previous thread was not holding a lock
    if lock_id == -1:
        return 0

    # Handle preemption if the previous thread is critical
    if is_critical_thread(prev, qnode):
        atomic_add(lock_info[lock_id].preempted_count, 2)
        qnode.is_critical_preempted = 1
